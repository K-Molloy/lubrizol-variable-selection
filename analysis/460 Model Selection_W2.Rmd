---
title: "460_ModelSelection_Week2"
author: "Geyi Liu"
date: "2020/11/26"
output:
  pdf_document: default
  html_document: default
---

```{r}
library(stringr)
library(dplyr)
library(ggplot2)
library(lubridate)
library(car)
data <- read.csv("../data/Data.csv", stringsAsFactors=FALSE)
data$LAB <- as.factor(data$LAB) # convert LAB to be a factor
data[is.na(data)] <- 0 # replace NAs with zero
```

```{r}
data_model <- data
data_model$X <- NULL # drop identifier column
data_model$LAB <- NULL # drop non-numeric LAB column
data_model <- data_model[1:nrow(data_model),477:ncol(data_model)]
Data_remove <- data_model[which(colMeans(data_model) > 0.02,)]# Remove 0 majority columns, because they're not statistically significant in linear analysis
```

```{r}
ggplot(data=data) +
  geom_boxplot(aes(x=LAB,y=Response,fill = LAB)) +
  labs(title="Boxplot of Response per LAB", 
       x="LAB", y="Response (%)")+
  theme_bw()+
  theme(panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        plot.title = element_text(hjust = 0.5))
# The Response distribution of three Labs are very similar
```

```{r}
# A linear regression model of the relationship between remain group variables and Response was established
# The difference in units between variables is not considered here (normalization)
model <- lm(Response~.,data = Data_remove)
summary(model)
# Multiple R-squared:  0.6621,	Adjusted R-squared:  0.653 
# The model can only explain 65% of the data
```

```{r}
# Too many variables cannot be processed with functions such as step()
# Here, we choose the statistically significant variables(p value<0.05) in the first model to continue building the model. This is a dangerous behavior, but it can help us to see which variables are significantly related to Response
# A preliminary conclusion can only be reached by manual treatment
model_2 <- lm(Response~Group1_9+Group1_11+Group1_20+Group1_22+Group2_4+
                Group2_9+Group2_14+Group2_22+Group3_8+Group3_10+Group3_12+
                Group11_6+Group6_6+Group9_24+Group9_25+Group9_26+Group9_28+
                Group13_6+Group13_11+Group13_13+Group13_20+Group13_21+
                Group13_22+Group13_34+Group13_46+Group13_47+Group13_49+
                Group13_50+Group13_52+Group13_56,data = Data_remove)
summary(model_2)
```


```{r}
# We need to reduce the number of variables as much as possible, and select the variables that have an significant effect on the modelling.
model_3 <- lm(Response~Group1_11+Group2_9+Group2_14+
                Group2_22+Group3_8+Group3_10+Group3_12+Group6_6+
                Group9_24+Group9_25+Group9_26+Group9_28+Group13_6+
                Group13_11+Group13_13+Group13_34+Group13_47+Group13_49+
                Group13_50+Group13_56,data = Data_remove)
# 20 variables
summary(model_3)
vif(model_3)
# # Residual histogram of model
resid=residuals(model_3)
hist(resid,breaks=40)
par(mfrow=c(2,2))
plot(model_3)
```


```{r}
# step function 
stp_model2 <- step(model_2)
summary(stp_model2)
```


```{r}
# According to the results of step (), we choose statistically significant variables to form a linear regression model.
model_4 <- lm(Response~Group1_9+Group1_11+Group1_22+
                Group2_9+Group2_14+Group2_22+Group3_8+Group3_10+Group3_12+
                Group11_6+Group6_6+Group9_24+Group9_25+Group9_26+Group9_28+
                Group13_6+Group13_11+Group13_13+Group13_20+Group13_34+
                Group13_46+Group13_47+Group13_49+
                Group13_50+Group13_52+Group13_56,data = Data_remove)
# Backward selection 
drop1(model_4,test='F')
# remove 13_46, 13_52
model_4_1 <- lm(Response~Group1_9+Group1_11+Group1_22+
                Group2_9+Group2_14+Group2_22+Group3_8+Group3_10+Group3_12+
                Group11_6+Group6_6+Group9_24+Group9_25+Group9_26+Group9_28+
                Group13_6+Group13_11+Group13_13+Group13_20+Group13_34+
                Group13_47+Group13_49+Group13_50+Group13_56
                ,data =Data_remove)
# 24 variables
drop1(model_4_1,test='F')
vif(model_4_1)
# 1_9,1_22,11_6,13_11 can be removed
# The result of backward selection is similar to that of manual selection
# The current linear model can only provide reference
```


```{r}
## PCA
# Next,  data frame composed of the variables in the previous linear regression model is used to continue the following principal component analysis
Data_PCA <- select(Data_remove,Group1_11,Group2_9,Group2_14,
                Group2_22,Group3_8,Group3_10,Group3_12,Group6_6,
                Group9_24,Group9_25,Group9_26,Group9_28,Group13_6,
                Group13_11,Group13_13,Group13_34,Group13_47,Group13_49,
                Group13_50,Group13_56,Response)
# The Response variable is removed before PCA
PCA <- princomp(Data_PCA[,-21],cor = TRUE)
summary(PCA)
# Standard deviation: Standard deviation
# Proportion of Variance: Contribution rate of each principal component
# Cumulative Proportion: Cumulative contribution rate of principal components

# Generally, the principal components with cumulative contribution rate over 85% are selected
# So we choose the first seven principal components
```

```{r}
library(corrplot)
# Correlation matrix
correlation <- cor(Data_PCA[,-21])
corrplot(correlation,)
# From this figure, we can see that the variables of Group 2,Group 3 and Group 9 are highly correlated
# The correlation between variables in different groups is not obvious
```


```{r}
# Output load matrix
# The linear relationship between principal components and original variables can be obtained
loadings(PCA)
# Scree plot
screeplot(PCA,type = 'lines',main = '')
```


Ridge Regression, LASSO, Logistic regression can also be used for regression modeling.
