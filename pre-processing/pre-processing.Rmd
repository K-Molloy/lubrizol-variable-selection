---
title: "pre-processing"
author: "E Magee"
date: "30 November 2020"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



```{r}
library(tidyverse) #loading in packages

raw_data = read.csv("../data/Data.csv") #reading in data
```

Looking at the data set the ingredients go up to 476 so assuming there are none missing, the first 477 columns can be omitted for the group data. The first column is an arbitrary number indicating the observation. There are 811 variable in total.

```{r}

raw_data[is.na(raw_data)] <- 0 #changes missing values to 0

no_ingredients = raw_data[, 478:811] #slicing the data set to get just groups, lab and response

#names(no_ingredients) #verifying that slicing worked as intended, commented out for the sake of the knitted html

```

Remove variables with more than 80% 0 values

```{r}
a = ncol(no_ingredients)
b = nrow(no_ingredients) * 0.8
c = c()

for (i in 1:a){
  
  if(sum(no_ingredients[,i]==0) >= b){
    c = c(c,i)
  }
}

no_ingredients = select(no_ingredients, -all_of(c))

```


Useful to standardise the data since the scales are very different across the data set. Don't want to standardise response, lab or pass though. Standardise columns other than these then add them to the data set.

```{r}

standardised_no_ingredients = data.frame(scale(no_ingredients[,1:163])) #standardising data.
standardised = standardised_no_ingredients

standardised_no_ingredients$Response = no_ingredients$Response
standardised_no_ingredients$Pass = as.numeric(no_ingredients$Response >= 50)

```

Discuss outliers. Maybe they shouldn't be removed since a particularly high measurement for one property could have a significant impact on the response variable. Also, variables that are mostly 0 value may bring up false outliers.

Do we want to do PCA? There are lots of variables so this might be good for dimensionality reduction.

```{r}

write.csv(standardised_no_ingredients, "../data/standardised_data_460.csv")

```

Wrote to csv, used PCA code from 403 labs to perform a preliminary PCA to check how appropriate it would be.

13 Principle Components are required to reach 85% explained variance.

22 are required for 90% and 31 for 95%

PCA does reduce the dimensionality but maybe not worth using for analysis since it loses interpretability and doesn't make the data much simpler to work with.

### Brief Exploratory with pre-processed data

```{r}

ggplot(standardised_no_ingredients, aes(x= Group1_11, y = Response))+
  geom_point(alpha = 0.1)+
  geom_hline(yintercept = 50, color = "red", linetype = "dashed")

```

looking at the above exploratory plot it agrees with Alex's finding that group1_11 is strongly correlated with the response. Since the value has a hard lower limit maybe normalisation would be more appropriate.



```{r}
library(BBmisc) #loading in package with a normalisation function
```

```{r}
#normalising the data between 0 and 1
normalised_no_ingredients = normalize(no_ingredients[,1:163], method = "range")

normalised_no_ingredients$Response = no_ingredients$Response
normalised_no_ingredients$Pass = as.numeric(no_ingredients$Response >= 50)

ggplot(normalised_no_ingredients, aes(x= Group1_11, y = Response))+
  geom_point(alpha = 0.1)+
  geom_hline(yintercept = 50, color = "red", linetype = "dashed")

```

Can see from this plot that the distribution doesn't change at all but the scale on x has changed. Not sure which one is more appropriate.


Writing the normalised data to a csv. Both normalised and standardised will be available for use later.
```{r}
write.csv(normalised_no_ingredients, "../data/normalised_data_460.csv")
```


```{r}
(sum(standardised > 6) + sum(standardised < -6))/sum(standardised < 100) * 100
```

0.13% of all values are outliers when variables have been removed