---
title: "Modelling - Ridge & Lasso Regression"
output:
  pdf_document: default
  html_document: default
---


```{r}
library(stringr)
library(dplyr)
library(ggplot2)
library(lubridate)
library(car)
library(tidyverse)
library(glmnet)
library(MLmetrics)

data <- read.csv("../data/standardised_data_460.csv", stringsAsFactors=FALSE)
```


# Ridge Regression

Making train_preds and test_preds store all attributes that could influence response.

```{r}
set.seed(100)
# Divide the data set  into training set and test set, the ratio is 7:3
train_sub = sample(nrow(data),7/10*nrow(data))
train_data = data[train_sub,]
train_preds = data.matrix(select(train_data, -c("Response")))

test_data = data[-train_sub,]
test_preds = data.matrix(select(test_data, -c("Response")))

```

Finding the lambda value that creates the lowest mean squared error using k fold cross validation.

```{r}
kfolds_cv_model <- 
  cv.glmnet(train_preds, train_data$Response, alpha = 0)

bestLambdaVal = kfolds_cv_model$lambda.min
```

Using the best lambda value found, we create a ridge regression model utilising all predictor variables.

```{r}
finalModel <- glmnet(train_preds, train_data$Response, alpha = 0, lambda = bestLambdaVal)
```

Predict the response variable using the model for each set of attribute values, then calculate the R^2 value to see the percentage of the variance which is explained by the model.

```{r}
predictedResponse <- predict(finalModel, s = bestLambdaVal, newx = test_preds)

sSE <- sum((predictedResponse - test_data$Response)^2)
sST <- sum((data$Response - mean(data$Response))^2)


rSQ <- 1 - sSE/sST
rSQ
```

Calculating the adjusted R-Squared value:
```{r}
n = dim(data)[1]
k = dim(data)[2]

adrSQ = 1 - ( (1 - rSQ)*(n-1) )/(n-k-1)
adrSQ
```










# Lasso Regression

Making std_data_predictors store all attributes that could influence response.

```{r}
set.seed(100)
# Divide the data set  into training set and test set, the ratio is 7:3
train_sub = sample(nrow(data),7/10*nrow(data))
train_data = data[train_sub,]
train_preds = data.matrix(select(train_data, -c("Response")))

test_data = data[-train_sub,]
test_preds = data.matrix(select(test_data, -c("Response")))

```

Finding the lambda value that creates the lowest mean squared error using k fold cross validation.

```{r}
kfolds_cv_model <- 
  cv.glmnet(train_preds, train_data$Response, alpha = 1)

bestLambdaVal = kfolds_cv_model$lambda.min
```

Using the best lambda value found, we create a lasso regression model utilising all predictor variables.

```{r}
finalModel <- glmnet(train_preds, train_data$Response, alpha = 1, lambda = bestLambdaVal)
```

Predict the response variable using the model for each set of attribute values, then calculate the R^2 value to see the percentage of the variance which is explained by the model.

```{r}
predictedResponse <- predict(finalModel, s = bestLambdaVal, newx = test_preds)

sSE <- sum((predictedResponse - test_data$Response)^2)
sST <- sum((data$Response - mean(data$Response))^2)


rSQ <- 1 - sSE/sST
rSQ
```

Calculating the adjusted R-Squared value:
```{r}
n = dim(data)[1]
k = dim(data)[2]

adrSQ = 1 - ( (1 - rSQ)*(n-1) )/(n-k-1)
adrSQ
```

