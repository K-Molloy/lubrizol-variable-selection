---
title: "Modelling - Ridge & Lasso Regression"
output:
  pdf_document: default
  html_document: default
---


```{r}
library(stringr)
library(dplyr)
library(ggplot2)
library(lubridate)
library(car)
library(tidyverse)
library(glmnet)
library(MLmetrics)

data <- read.csv("../data/standardised_data_460.csv", stringsAsFactors=FALSE)
data = select(data, -c(Pass))
```


# Ridge Regression

Making train_preds and test_preds store all attributes that could influence response.

```{r}
set.seed(580)
# Divide the data set  into training set and test set, the ratio is 7:3

train_preds = data.matrix(select(data, -c("Response")))
```

Finding the lambda value that creates the lowest mean squared error using k fold cross validation.

```{r}
kfolds_cv_model <- 
  cv.glmnet(train_preds, data$Response, alpha = 0)

bestLambdaVal = kfolds_cv_model$lambda.min
```

Using the best lambda value found, we create a ridge regression model utilising all predictor variables.

```{r}
finalModel <- glmnet(train_preds, data$Response, alpha = 0, lambda = bestLambdaVal)
```

Predict the response variable using the model for each set of attribute values, then calculate the R^2 value to see the percentage of the variance which is explained by the model.

```{r}
predictedResponse <- predict(finalModel, s = bestLambdaVal, newx = train_preds)

sSE <- sum((predictedResponse - data$Response)^2)
sST <- sum((data$Response - mean(data$Response))^2)


rSQ <- 1 - sSE/sST
rSQ
```

Calculating the adjusted R-Squared value:
```{r}
n = dim(data)[1]
k = dim(data)[2]

adrSQ = 1 - ( (1 - rSQ)*(n-1) )/(n-k-1)
adrSQ
mse = MSE(predictedResponse, data$Response)
mse
mae = MAE(predictedResponse, data$Response)
mae
```
As expected the adjusted R-squared is lower than the normal r-squared value, however the decrease is such that the model is still competitive in its closeness of fit to other models.

```{r}
ggplot() + geom_point(aes(y=predictedResponse, x=data$Response)) +
  geom_abline(slope=1, color="red")
```




# Lasso Regression

Making std_data_predictors store all attributes that could influence response.

```{r}
data <- read.csv("../data/standardised_data_460.csv", stringsAsFactors=FALSE)
data = select(data, -c(Pass))
set.seed(580)
# Divide the data set  into training set and test set, the ratio is 7:3
train_preds = data.matrix(select(data, -c("Response")))

```

Finding the lambda value that creates the lowest mean squared error using k fold cross validation.

```{r}
kfolds_cv_model <- 
  cv.glmnet(train_preds, data$Response, alpha = 1)

bestLambdaVal = kfolds_cv_model$lambda.min
```

Using the best lambda value found, we create a lasso regression model utilising all predictor variables.

```{r}
finalModel <- glmnet(train_preds, data$Response, alpha = 1, lambda = bestLambdaVal)
```

Predict the response variable using the model for each set of attribute values, then calculate the R^2 value to see the percentage of the variance which is explained by the model.

```{r}
predictedResponse <- predict(finalModel, s = bestLambdaVal, newx = train_preds)

sSE <- sum((predictedResponse - data$Response)^2)
sST <- sum((data$Response - mean(data$Response))^2)


rSQ <- 1 - sSE/sST
rSQ
```

Calculating the adjusted R-Squared value:
```{r}
n = dim(data)[1]
k = dim(data)[2]

adrSQ = 1 - ( (1 - rSQ)*(n-1) )/(n-k-1)
adrSQ
allAttributeAdrSQ = adrSQ
allAttributeMSE = MSE(predictedResponse, data$Response)
allAttributeMSE
mae = MAE(predictedResponse, data$Response)
mae
```
The adjusted R-squared value is marginally better than that generated by ridge regression, however the similarity of approach is highlighted by the closeness in quality of fit.


Finding the coefficient values of each attribute in the data, I have taken the highest ten attributes and used them in a new model below.
```{r}

coefs = coef(finalModel)
coefs = as.matrix(coefs)


#coefs = coefs[order(coefs$s0),]

#coefs
#coefs = data.frame(attributes = coefs_names, vals = coefs_vals)

```

```{r}
ggplot() + geom_point(aes(y=predictedResponse, x=data$Response)) +
  geom_abline(slope=1, color="red")
```







# Lasso Regression - Reduced to 10 Attributes

Making data store the top ten variables that help to explain the variation in the data based on the previous lasso model utilising all attributes of the data.

```{r}
data <- read.csv("../data/standardised_data_460.csv", stringsAsFactors=FALSE)
data = select(data, -c(Pass))
set.seed(580)

data = select(data, c("Group1_11", "Group13_34", "Group2_9", "Group13_5",
                      "Group2_21", "Group2_45", "Group2_18", "Group9_31",
                      "Group8_7", "Group3_9", "Response"))

# Divide the data set  into training set and test set, the ratio is 7:3
train_preds = data.matrix(select(data, -c("Response")))

```

Finding the lambda value that creates the lowest mean squared error using k fold cross validation.

```{r}
kfolds_cv_model <- 
  cv.glmnet(train_preds, data$Response, alpha = 1)

bestLambdaVal = kfolds_cv_model$lambda.min
```

Using the best lambda value found, we create a lasso regression model utilising all predictor variables.

```{r}
finalModel <- glmnet(train_preds, data$Response, alpha = 1, lambda = bestLambdaVal)
```

Predict the response variable using the model for each set of attribute values, then calculate the R^2 value to see the percentage of the variance which is explained by the model.

```{r}
predictedResponse <- predict(finalModel, s = bestLambdaVal, newx = train_preds)

sSE <- sum((predictedResponse - data$Response)^2)
sST <- sum((data$Response - mean(data$Response))^2)


rSQ <- 1 - sSE/sST
rSQ
```

Calculating the adjusted R-Squared value:
```{r}
n = dim(data)[1]
k = dim(data)[2]

adrSQTEN = 1 - ( (1 - rSQ)*(n-1) )/(n-k-1)
adrSQTEN
mse = MSE(predictedResponse, data$Response)
mse
mae = MAE(predictedResponse, data$Response)
mae
```
The adjusted r-squared of this model shows that the ten attributes utilised in this model ("Group1_11", "Group13_34", "Group2_9", "Group13_5", "Group2_21", "Group2_45", "Group2_18", "Group9_31", "Group8_7", "Group3_9") explains `r adrSQTEN` of the variation in the Response variable, this is important to note given that the model using all attributes explained `r allAttributeAdrSQ` of the variation.

```{r}
ggplot() + geom_point(aes(y=predictedResponse, x=data$Response)) +
  geom_abline(slope=1, color="red")
```





## Lasso Regression - Determining Top 10 Variables Impacts

In this section I will use a model containing all attributes, except I will remove one of the previously explored ten attributes to analyses the individual impact of removal on mean squared error (MSE).

```{r}
data <- read.csv("../data/standardised_data_460.csv", stringsAsFactors=FALSE)
data = select(data, -c(Pass))
set.seed(580)

attributeToRemove = c("Group1_11", "Group13_34", "Group2_9", "Group13_5", "Group2_21",
                      "Group2_45", "Group2_18", "Group9_31", "Group8_7", "Group3_9")

changeInRSQ = c()
changeInMSE = c()
changeInMAE = c()

for (i in 1:10){
  newData = select(data, -c(attributeToRemove[i]))
  
  # Divide the data set  into training set and test set, the ratio is 7:3
  train_preds = data.matrix(select(newData, -c("Response")))
  
  kfolds_cv_model <- 
    cv.glmnet(train_preds, newData$Response, alpha = 1)
  
  bestLambdaVal = kfolds_cv_model$lambda.min
  
  finalModel <- glmnet(train_preds, newData$Response, alpha = 1,
                       lambda = bestLambdaVal)
  
  predictedResponse <- predict(finalModel, s = bestLambdaVal, newx = train_preds)
  
  sSE <- sum((predictedResponse - data$Response)^2)
  sST <- sum((newData$Response - mean(newData$Response))^2)
  
  
  rSQ <- 1 - sSE/sST
  rSQ
  
  n = dim(newData)[1]
  k = dim(newData)[2]
  
  adrSQ = 1 - ( (1 - rSQ)*(n-1) )/(n-k-1)
  
  changeInRSQ[i] = adrSQ - allAttributeAdrSQ
  changeInMSE[i] = MSE(predictedResponse, data$Response) - allAttributeMSE
  changeInMAE[i] = MAE(predictedResponse, data$Response)
}
```

From this graph we can see that the results of removing the ten explored variables on MSE is mixed, with most showing that removing the attribute caused MSE to decrease in comparison to a model utilising all variables. This is intuitive as we would always anticipate more information improves the models closeness of fit, as the coefficient of completely unrelated attributes would be minimised to become unimpactfull.
```{r}
effectOfRemoval = data.frame(name=attributeToRemove,
                             changeRSQ=changeInRSQ,
                             changeMSE=changeInMSE)

ggplot(effectOfRemoval) + geom_col(aes(x=changeMSE, y=name)) +
  labs(y="Removed Attribute", x="Change in MSE.")
```















## Lasso Regression - Determining The MSE Impact of Each Variable
Here I am recording the effect that removing each variable has from the model, as it may provide insight into how to extract performance from the model by highlighting influential attributes.
```{r}
data <- read.csv("../data/standardised_data_460.csv", stringsAsFactors=FALSE)
data = select(data, -c(Pass))
set.seed(580)

attributeToRemove = names(data)
attributeToRemove = attributeToRemove[attributeToRemove %in% "Response" == FALSE]

changeInRSQ = c()
changeInMSE = c()
changeInMAE = c()

for (i in 1:length(attributeToRemove)){
  newData = select(data, -c(attributeToRemove[i]))
  
  # Divide the data set  into training set and test set, the ratio is 7:3
  train_preds = data.matrix(select(newData, -c("Response")))
  
  kfolds_cv_model <- 
    cv.glmnet(train_preds, newData$Response, alpha = 1)
  
  bestLambdaVal = kfolds_cv_model$lambda.min
  
  finalModel <- glmnet(train_preds, newData$Response, alpha = 1,
                       lambda = bestLambdaVal)
  
  predictedResponse <- predict(finalModel, s = bestLambdaVal, newx = train_preds)
  
  sSE <- sum((predictedResponse - newData$Response)^2)
  sST <- sum((newData$Response - mean(newData$Response))^2)
  
  
  rSQ <- 1 - sSE/sST
  rSQ
  
  n = dim(newData)[1]
  k = dim(newData)[2]
  
  adrSQ = 1 - ( (1 - rSQ)*(n-1) )/(n-k-1)
  
  changeInRSQ[i] = adrSQ - allAttributeAdrSQ
  changeInMSE[i] = MSE(predictedResponse, newData$Response) - allAttributeMSE
  changeInMAE[i] = MAE(predictedResponse, newData$Response)
}
```

```{r}
effectOfRemovalOG = data.frame(name=attributeToRemove,
                             changeRSQ=changeInRSQ,
                             changeMSE=changeInMSE)

effectOfRemoval = arrange(effectOfRemovalOG, desc(changeMSE))

ggplot(effectOfRemoval) + geom_col(aes(x=changeMSE, y=name)) +
  labs(y="Removed Attribute", x="Change in MSE.")
```

Finding the attributes that have a positive effect on MSE:
```{r}
positiveMSE = filter(effectOfRemovalOG, changeMSE > 0)

ggplot(positiveMSE) + geom_col(aes(x=changeMSE, y=name)) +
  labs(y="Removed Attribute", x="Change in MSE.")
```
As the graph shows there are still too many variables (`r dim(positiveMSE)[1]`) to be easily interpretable, so I will attempt to reduce them further while maintaining an explanatory effect over Response.

```{r}
variables = positiveMSE$name

variables = as.character(variables)
variables[length(variables)+1] = "Response"
```

```{r}
data <- read.csv("../data/standardised_data_460.csv", stringsAsFactors=FALSE)
data = select(data, -c(Pass))
set.seed(580)

attributeToRemove = variables
newData = select(data, c(attributeToRemove))

# Divide the data set  into training set and test set, the ratio is 7:3
train_preds = data.matrix(select(newData, -c("Response")))

kfolds_cv_model <- 
  cv.glmnet(train_preds, newData$Response, alpha = 1)

bestLambdaVal = kfolds_cv_model$lambda.min

finalModel <- glmnet(train_preds, newData$Response, alpha = 1, lambda = bestLambdaVal)

predictedResponse <- predict(finalModel, s = bestLambdaVal, newx = train_preds)

sSE <- sum((predictedResponse - newData$Response)^2)
sST <- sum((newData$Response - mean(newData$Response))^2)


rSQ <- 1 - sSE/sST

n = dim(newData)[1]
k = dim(newData)[2]

adrSQ = 1 - ( (1 - rSQ)*(n-1) )/(n-k-1)
adrSQ
mse = MSE(predictedResponse, newData$Response)
mse
mae = MAE(predictedResponse, newData$Response)
mae
```

When comparing the above lasso regression model that utilises `r dim(positiveMSE)[1]-10` more variables than the initial 10 variable approach using lasso, the difference between the two models adjusted r-squared values is only `r adrSQ - adrSQTEN`. This further highlights the impact that the ten variables ("Group1_11", "Group13_34", "Group2_9", "Group13_5", "Group2_21", "Group2_45", "Group2_18", "Group9_31", "Group8_7", "Group3_9") have on the response variable.

```{r}
ggplot() + geom_point(aes(y=predictedResponse, x=newData$Response)) +
  geom_abline(slope=1, color="red")
```
